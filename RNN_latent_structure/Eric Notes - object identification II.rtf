{\rtf1\ansi\ansicpg1252\cocoartf1671\cocoasubrtf200
{\fonttbl\f0\fswiss\fcharset0 Helvetica-Bold;\f1\fswiss\fcharset0 Helvetica;\f2\fswiss\fcharset0 Helvetica-BoldOblique;
\f3\fswiss\fcharset0 Helvetica-Oblique;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red251\green2\blue7;\red5\green68\blue254;
}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;\cssrgb\c100000\c14913\c0;\cssrgb\c0\c38136\c99824;
}
\margl1440\margr1440\vieww14140\viewh13040\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\ri-2900\pardirnatural\partightenfactor0

\f0\b\fs32 \cf0 Eric Shea-Brown & Stefano \cf2 \expnd0\expndtw0\kerning0
Recanatesi
\f1\b0 \cf0 \kerning1\expnd0\expndtw0 \
\'93Identification of Objects as Latent Variables in Static Visual Scenes\'94\
Feb. 8, 2019\
\
\
\ul Overall Approach\ulnone \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\ri-2900\pardirnatural\partightenfactor0

\f2\i\b \cf0 Goal
\f1\i0\b0 : to generalize the RNN framework developed by Stefano and Eric to the case of learning objects as a key component in the latent space of static image ensembles explored by eye movements\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\ri-2900\pardirnatural\partightenfactor0
\cf0 \ul \ulc0 Ingredients\ulnone \
1) Choose a set of 2D images\
2) Choose statistics for eye movements\
3) Choose a method for encoding the 2D image into a set of neural activity\
\
\ul Task\ulnone \
1) Build an RNN with two kinds of input:\
	A. Exact copy of the eye movements vector at time, t\
	B. Population activity from the encoding layer of neurons at time, t\

\f3\i Note
\f1\i0 : inputs arrive in each RNN unit through fixed, random-weight feedforward synapses that randomly combine the eye movement vector and encoding layer\
\
\cf3 r_t = g(W*r_\{t-1\} + W_in*o_t)\
\
Where W_in = randn() \'97> N(0,1)?\
encoding layer? I assume this just means the input observations from the random walk? Or is there some kind of manipulation performed to the observations?\cf0 \
\
\
2) Train the RNN to predict the population activity of the encoding layer at time, t+1\
	\'95 use backpropgation algorithm\
	\'95 create a readout layer with fixed, random-weight output synapses from RNN\
	\'95 each readout unit is assigned to predict one unit in the encoding layer\
	\'95 define error as sum of squared difference between readout & encoding neurons\
\
\
\ul Choice of Eye Movements\ulnone \
\
Fixational eye movements: random walk\
\'95\'a0take parameters from Xaq\'92s paper\
\cf3 I think I found the paper of interest.\cf0 \
\
Saccades: choose random vectors\
\'95 take some statistics from papers\
\'95 choose a time interval: e.g., uniform in the range of 0.25 \'96 0.5 sec\
\'95 choose a random direction: uniform over 360\'b0\
\'95 choose a random amplitude: use data from papers\
\cf3 randomly jump between different points of fixation within the image; the time interval is the length between jumps.\cf0 \
\
Makes no sense to get any more complicated, as neuroscience cannot predict human or monkey saccades with any satisfaction\
\

\f3\i Overall structure
\f1\i0 : \
\'95\'a0for each image, the eye movement trajectory \'93looks at it\'94\
\'95 \'93looking at it\'94 means choose a sequence of 5-10 saccades from that ensemble\
\cf3 For each image take 5-10 random movements across the image.\cf0 \
\

\f3\i Technical issue:
\f1\i0  what if your randomly chosen movement takes you out of image?\
\'95 approach #1: wrap-around boundary conditions\
  \'96 in this case, you can make the first saccade to a random location\
\cf3 Have the eye movement go to other side of image.\cf0 \
\
\'95 approach #2: if movement takes you outside the 2D image, then choose again\
  \'96 in this case, you should probably make the first saccade to the image center\'85\
\
\
\ul Choice of the Image Ensemble\ulnone \
\
Level 1: grey background, add chosen \'93objects\'94 by occlusion (\cf3 paste on top\cf0 ); greyscale images\
\'95 possible choice of objects: dead leaves model?\
\cf3 randomly place a series of objects onto the plane and superimpose new ones on top of old. We can adjust parameters of the objects to change their shape/size. For example, we could have a sequence of circles placed on some background and randomly choose their radius.\cf0 \
\
\'95 better idea: complex objects, like in monkey experiments on IT cortex\
  \'96 examples: fruit, tools, cars, faces, etc.\
\
Level 2: natural image with objects pasted on (\'93Where is Waldo?\'94)\
\'95 RNN should learn some objects from the actual scene (not pasted on)\
\
\
Image Ensemble:\
\'95 choose a larger set of objects, say 100\
\'95 for each image, randomly choose to display a subset, say 50\
\cf3 Choose some subset of objects to display on a blank background. Randomly place them and vary parameters of the object (i.e. size, location, etc)\cf0 \
  \'96 
\f3\i maybe
\f1\i0 : ask an IT lab if we can use their image set??\
\'95 each object, i, has: \
  \'96 random location, \{x_i, y_i\}\
  \'96 random size, s_i\
  \'96 
\f3\i maybe
\f1\i0 : random depth, z_i (used to determine occlusion)\
\cf3 or just randomly order the placement of the objects. This will generate the same concept of depth.\cf0 \
  \'96 
\f3\i maybe
\f1\i0 : random rotation, theta_i\
\

\f3\i Basic logic:
\f1\i0  the image ensemble presents a fixed, discrete set of objects, each having a range of continuous variables\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\ri-2900\pardirnatural\partightenfactor0

\f2\i\b \cf0 Hypothesis #1
\f1\i0\b0 : units in trained RNN learn to encode the continuous variables?\

\f2\i\b Hypothesis #2
\f1\i0\b0 : units in trained RNN (also) learn to encode the object index?\
\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\ri-2900\pardirnatural\partightenfactor0
\cf0 \ul Choices of Sensory Encoding\ulnone \
\
Level 1: mosaic array of fixed spatial filters\
\'95 no explicit temporal dynamics (i.e. all dynamics driven by eye movements)\
\'95 center-surround filters\
\'95 like one type of retinal ganglion cell\
\'95 also like a convolutional network\
\cf3 Just have some array of points around the fixation point of the eye where pixel values are read from during the experiment.\cf0 \
\
Level 2: give each ganglion cell spatial and temporal dynamics, like real cells\
\'95 qualitative difference: temporal dynamics have delays; effect on predictions?\
\'95 space and time are separable\
\'95 like the receptive field center mechanism (i.e. not so realistic for the surround)\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\ri-2900\pardirnatural\partightenfactor0
\cf4 [MJB: We never discussed this point on Wed.  I am inclined to think this encoding delay does not matter, because the RNN will try to predict the activity of the encoding layer itself at time, t+1, rather than the stimulus.]\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\ri-2900\pardirnatural\partightenfactor0
\cf3 The observations come with some sort of a delay. At each point, record what the ganglion is seeing but do not present it until a time step later.\cf0 \
\

\f3\i Upshot
\f1\i0 :\'a0I think I prefer Level 1\
\

\f3\i Issue
\f1\i0 : if the encoding layer has a perfectly ordered array of identical spatial filters, then maybe the readouts from the RNN should also have some repeated, convolutional structure??\
\\\
\
\
\
\
\
\
\
\
\
\
\
}